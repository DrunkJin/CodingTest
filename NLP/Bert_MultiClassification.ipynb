{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33cdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c966160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d82e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\r\n",
      "---------------------------- --------------------\r\n",
      "absl-py                      1.0.0\r\n",
      "accelerate                   0.16.0\r\n",
      "aiohttp                      3.8.1\r\n",
      "aiosignal                    1.2.0\r\n",
      "alembic                      1.8.1\r\n",
      "anaconda                     0.0.1.1\r\n",
      "anyio                        3.5.0\r\n",
      "appdirs                      1.4.4\r\n",
      "argon2-cffi                  21.3.0\r\n",
      "argon2-cffi-bindings         21.2.0\r\n",
      "astor                        0.8.1\r\n",
      "asttokens                    2.0.5\r\n",
      "astunparse                   1.6.3\r\n",
      "async-timeout                4.0.2\r\n",
      "attrs                        21.4.0\r\n",
      "autoviz                      0.1.58\r\n",
      "Babel                        2.9.1\r\n",
      "backcall                     0.2.0\r\n",
      "bleach                       4.1.0\r\n",
      "blinker                      1.4\r\n",
      "blis                         0.7.9\r\n",
      "bokeh                        2.4.3\r\n",
      "Boruta                       0.3\r\n",
      "boto3                        1.15.18\r\n",
      "botocore                     1.18.18\r\n",
      "cachetools                   5.0.0\r\n",
      "catalogue                    1.0.2\r\n",
      "certifi                      2021.10.8\r\n",
      "cffi                         1.15.0\r\n",
      "chainer                      7.8.1\r\n",
      "chardet                      3.0.4\r\n",
      "charset-normalizer           2.0.12\r\n",
      "click                        8.1.2\r\n",
      "clikit                       0.6.2\r\n",
      "cloudpickle                  2.0.0\r\n",
      "cmdstanpy                    0.9.5\r\n",
      "colorcet                     3.0.1\r\n",
      "colorlover                   0.3.0\r\n",
      "conda                        4.3.16\r\n",
      "config                       0.5.1\r\n",
      "convertdate                  2.4.0\r\n",
      "crashtest                    0.3.1\r\n",
      "crc32c                       2.3\r\n",
      "cryptography                 2.8\r\n",
      "cufflinks                    0.17.3\r\n",
      "cupshelpers                  1.0\r\n",
      "cupy                         10.2.0\r\n",
      "cycler                       0.11.0\r\n",
      "cymem                        2.0.7\r\n",
      "Cython                       0.29.28\r\n",
      "dask                         2022.12.0\r\n",
      "databricks-cli               0.17.3\r\n",
      "dataclasses                  0.6\r\n",
      "dataprep                     0.4.5\r\n",
      "datasets                     1.11.0\r\n",
      "dbus-python                  1.2.16\r\n",
      "debugpy                      1.5.1\r\n",
      "decorator                    5.1.1\r\n",
      "defer                        1.0.6\r\n",
      "defusedxml                   0.7.1\r\n",
      "descartes                    1.1.0\r\n",
      "diffusers                    0.3.0\r\n",
      "dill                         0.3.5.1\r\n",
      "distro                       1.4.0\r\n",
      "docker                       6.0.1\r\n",
      "easydict                     1.10\r\n",
      "emoji                        1.6.3\r\n",
      "entrypoints                  0.4\r\n",
      "enum34                       1.1.10\r\n",
      "ephem                        4.1.4\r\n",
      "et-xmlfile                   1.1.0\r\n",
      "etils                        0.8.0\r\n",
      "executing                    0.8.3\r\n",
      "ez-setup                     0.9\r\n",
      "fastrlock                    0.8\r\n",
      "fasttext                     0.9.2\r\n",
      "filelock                     3.6.0\r\n",
      "Flask                        2.2.2\r\n",
      "Flask-Cors                   3.0.10\r\n",
      "flatbuffers                  2.0\r\n",
      "fonttools                    4.29.1\r\n",
      "frozenlist                   1.3.0\r\n",
      "fsspec                       2022.3.0\r\n",
      "ftfy                         6.1.1\r\n",
      "funcy                        1.17\r\n",
      "future                       0.18.2\r\n",
      "gast                         0.5.3\r\n",
      "gensim                       3.8.3\r\n",
      "gitdb                        4.0.10\r\n",
      "GitPython                    3.1.29\r\n",
      "gluonnlp                     0.10.0\r\n",
      "google-auth                  2.6.0\r\n",
      "google-auth-oauthlib         0.4.6\r\n",
      "google-pasta                 0.2.0\r\n",
      "googleapis-common-protos     1.56.4\r\n",
      "graphviz                     0.8.4\r\n",
      "greenlet                     1.1.2\r\n",
      "grpcio                       1.44.0\r\n",
      "gunicorn                     20.1.0\r\n",
      "h5py                         3.6.0\r\n",
      "hijri-converter              2.2.4\r\n",
      "holidays                     0.20\r\n",
      "holoviews                    1.14.9\r\n",
      "htmlmin                      0.1.12\r\n",
      "httplib2                     0.14.0\r\n",
      "httpstan                     4.9.1\r\n",
      "huggingface-hub              0.11.1\r\n",
      "hvplot                       0.8.2\r\n",
      "hyperopt                     0.2.7\r\n",
      "idna                         3.3\r\n",
      "ImageHash                    4.2.1\r\n",
      "imageio                      2.16.1\r\n",
      "imbalanced-learn             0.7.0\r\n",
      "implicit                     0.5.2\r\n",
      "importlib-metadata           4.11.2\r\n",
      "importlib-resources          5.4.0\r\n",
      "install                      1.3.5\r\n",
      "ipykernel                    6.9.1\r\n",
      "ipython                      8.1.1\r\n",
      "ipython-genutils             0.2.0\r\n",
      "ipython-sql                  0.4.0\r\n",
      "ipywidgets                   7.6.5\r\n",
      "itsdangerous                 2.1.2\r\n",
      "jedi                         0.18.1\r\n",
      "Jinja2                       3.0.3\r\n",
      "jmespath                     0.10.0\r\n",
      "joblib                       1.1.0\r\n",
      "JPype1                       1.3.0\r\n",
      "json5                        0.9.6\r\n",
      "jsonlines                    3.1.0\r\n",
      "jsonpath-ng                  1.5.3\r\n",
      "jsonschema                   4.4.0\r\n",
      "jupyter                      1.0.0\r\n",
      "jupyter-client               7.1.2\r\n",
      "jupyter-console              6.4.3\r\n",
      "jupyter-core                 4.9.2\r\n",
      "jupyter-server               1.13.5\r\n",
      "jupyterlab                   3.3.1\r\n",
      "jupyterlab-pygments          0.1.2\r\n",
      "jupyterlab-server            2.10.3\r\n",
      "jupyterlab-widgets           1.0.2\r\n",
      "jupyterthemes                0.20.0\r\n",
      "keras                        2.8.0\r\n",
      "Keras-Preprocessing          1.1.2\r\n",
      "keyring                      18.0.1\r\n",
      "kiwisolver                   1.3.2\r\n",
      "kmodes                       0.12.2\r\n",
      "kobert                       0.2.3\r\n",
      "kobert-tokenizer             0.1\r\n",
      "konlpy                       0.6.0\r\n",
      "korean-lunar-calendar        0.3.1\r\n",
      "Korpora                      0.2.0\r\n",
      "kss                          3.6.4\r\n",
      "language-selector            0.1\r\n",
      "launchpadlib                 1.10.13\r\n",
      "lazr.restfulclient           0.14.2\r\n",
      "lazr.uri                     1.0.3\r\n",
      "lesscpy                      0.15.0\r\n",
      "libclang                     13.0.0\r\n",
      "lightfm                      1.16\r\n",
      "lightgbm                     3.3.3\r\n",
      "llvmlite                     0.37.0\r\n",
      "locket                       1.0.0\r\n",
      "LunarCalendar                0.0.9\r\n",
      "lxml                         4.8.0\r\n",
      "macaroonbakery               1.3.1\r\n",
      "Mako                         1.2.4\r\n",
      "Markdown                     3.3.6\r\n",
      "MarkupSafe                   2.1.1\r\n",
      "marshmallow                  3.19.0\r\n",
      "matplotlib                   3.5.1\r\n",
      "matplotlib-inline            0.1.3\r\n",
      "mecab                        0.996.3\r\n",
      "Metaphone                    0.6\r\n",
      "missingno                    0.5.1\r\n",
      "mistune                      0.8.4\r\n",
      "mizani                       0.7.4\r\n",
      "mlflow                       2.0.1\r\n",
      "mlxtend                      0.19.0\r\n",
      "more-itertools               9.0.0\r\n",
      "multidict                    6.0.2\r\n",
      "multimethod                  1.8\r\n",
      "multiprocess                 0.70.13\r\n",
      "murmurhash                   1.0.9\r\n",
      "mxnet                        1.7.0.post2\r\n",
      "mxnet-cu112                  1.9.0\r\n",
      "nbclassic                    0.3.6\r\n",
      "nbclient                     0.5.12\r\n",
      "nbconvert                    6.4.2\r\n",
      "nbformat                     5.1.3\r\n",
      "nest-asyncio                 1.5.4\r\n",
      "networkx                     2.7.1\r\n",
      "nltk                         3.7\r\n",
      "notebook                     6.4.8\r\n",
      "notebook-shim                0.1.0\r\n",
      "numba                        0.54.1\r\n",
      "numexpr                      2.8.4\r\n",
      "numpy                        1.23.5\r\n",
      "oauthlib                     3.2.0\r\n",
      "onnx                         1.11.0\r\n",
      "onnxruntime                  1.8.0\r\n",
      "onnxruntime-gpu              1.10.0\r\n",
      "openpyxl                     3.0.9\r\n",
      "opt-einsum                   3.3.0\r\n",
      "packaging                    21.3\r\n",
      "paddlepaddle-gpu             2.2.2\r\n",
      "palettable                   3.3.0\r\n",
      "pandas                       1.4.1\r\n",
      "pandas-datareader            0.10.0\r\n",
      "pandas-profiling             3.2.0\r\n",
      "pandasql                     0.7.3\r\n",
      "pandocfilters                1.5.0\r\n",
      "panel                        0.12.7\r\n",
      "param                        1.12.2\r\n",
      "parso                        0.8.3\r\n",
      "partd                        1.3.0\r\n",
      "pastel                       0.2.1\r\n",
      "patsy                        0.5.2\r\n",
      "pexpect                      4.8.0\r\n",
      "phik                         0.12.2\r\n",
      "pickleshare                  0.7.5\r\n",
      "Pillow                       9.0.1\r\n",
      "pip                          23.0.1\r\n",
      "plac                         1.1.3\r\n",
      "plotly                       5.13.0\r\n",
      "plotnine                     0.8.0\r\n",
      "ply                          3.11\r\n",
      "preshed                      3.0.8\r\n",
      "prettytable                  0.7.2\r\n",
      "prometheus-client            0.13.1\r\n",
      "promise                      2.3\r\n",
      "prompt-toolkit               3.0.28\r\n",
      "prophet                      1.1.2\r\n",
      "protobuf                     3.19.4\r\n",
      "psutil                       5.9.4\r\n",
      "psycopg2-binary              2.9.3\r\n",
      "ptyprocess                   0.7.0\r\n",
      "pure-eval                    0.2.2\r\n",
      "py4j                         0.10.9.3\r\n",
      "pyamg                        4.2.3\r\n",
      "pyarrow                      10.0.0\r\n",
      "pyasn1                       0.4.8\r\n",
      "pyasn1-modules               0.2.8\r\n",
      "pybind11                     2.9.2\r\n",
      "pycairo                      1.16.2\r\n",
      "pycaret                      2.3.10\r\n",
      "pycosat                      0.6.3\r\n",
      "pycparser                    2.21\r\n",
      "pyct                         0.4.8\r\n",
      "pycups                       1.9.73\r\n",
      "pydantic                     1.9.0\r\n",
      "pyDeprecate                  0.3.2\r\n",
      "pydot                        1.4.2\r\n",
      "Pygments                     2.11.2\r\n",
      "PyGObject                    3.36.0\r\n",
      "PyJWT                        1.7.1\r\n",
      "pyLDAvis                     3.2.2\r\n",
      "pylev                        1.4.0\r\n",
      "pymacaroons                  0.13.0\r\n",
      "PyMeeus                      0.5.12\r\n",
      "PyMySQL                      1.0.2\r\n",
      "PyNaCl                       1.3.0\r\n",
      "pynndescent                  0.5.8\r\n",
      "pyod                         1.0.6\r\n",
      "pyparsing                    3.0.7\r\n",
      "pyRFC3339                    1.1\r\n",
      "pyrsistent                   0.18.1\r\n",
      "pysimdjson                   5.0.2\r\n",
      "pyspark                      3.2.1\r\n",
      "pystan                       3.6.0\r\n",
      "python-apt                   2.0.0+ubuntu0.20.4.7\r\n",
      "python-crfsuite              0.9.8\r\n",
      "python-dateutil              2.8.2\r\n",
      "python-mecab-kor             1.2.3\r\n",
      "python-stdnum                1.18\r\n",
      "pytorch-lightning            1.6.3\r\n",
      "pytz                         2021.3\r\n",
      "pyviz-comms                  2.2.1\r\n",
      "PyWavelets                   1.2.0\r\n",
      "PyYAML                       5.4.1\r\n",
      "pyzmq                        22.3.0\r\n",
      "qtconsole                    5.2.2\r\n",
      "QtPy                         2.0.1\r\n",
      "querystring-parser           1.2.4\r\n",
      "rapidfuzz                    2.13.6\r\n",
      "recsys                       0.0.4\r\n",
      "regex                        2021.11.10\r\n",
      "requests                     2.27.1\r\n",
      "requests-oauthlib            1.3.1\r\n",
      "requests-unixsocket          0.2.0\r\n",
      "responses                    0.18.0\r\n",
      "rsa                          4.8\r\n",
      "ruamel.yaml                  0.17.21\r\n",
      "ruamel.yaml.clib             0.2.7\r\n",
      "s3transfer                   0.3.7\r\n",
      "sacremoses                   0.0.49\r\n",
      "safetensors                  0.2.8\r\n",
      "scikit-image                 0.19.2\r\n",
      "scikit-learn                 0.23.2\r\n",
      "scikit-plot                  0.3.7\r\n",
      "scikit-surprise              1.1.1\r\n",
      "scipy                        1.9.3\r\n",
      "seaborn                      0.11.2\r\n",
      "SecretStorage                2.3.1\r\n",
      "Send2Trash                   1.8.0\r\n",
      "sentencepiece                0.1.91\r\n",
      "setuptools                   60.10.0\r\n",
      "setuptools-git               1.2\r\n",
      "shap                         0.41.0\r\n",
      "simplejson                   3.16.0\r\n",
      "six                          1.16.0\r\n",
      "slicer                       0.0.7\r\n",
      "smart-open                   5.2.1\r\n",
      "smmap                        5.0.0\r\n",
      "sniffio                      1.2.0\r\n",
      "soynlp                       0.0.493\r\n",
      "spacy                        2.3.8\r\n",
      "SQLAlchemy                   1.3.24\r\n",
      "sqlparse                     0.4.2\r\n",
      "srsly                        1.0.6\r\n",
      "stack-data                   0.2.0\r\n",
      "statsmodels                  0.13.2\r\n",
      "surprise                     0.1\r\n",
      "systemd-python               234\r\n",
      "tabulate                     0.9.0\r\n",
      "tangled-up-in-unicode        0.2.0\r\n",
      "tenacity                     8.0.1\r\n",
      "tensorboard                  2.8.0\r\n",
      "tensorboard-data-server      0.6.1\r\n",
      "tensorboard-plugin-wit       1.8.1\r\n",
      "tensorflow-datasets          4.6.0\r\n",
      "tensorflow-gpu               2.8.0\r\n",
      "tensorflow-io-gcs-filesystem 0.24.0\r\n",
      "tensorflow-metadata          1.10.0\r\n",
      "termcolor                    1.1.0\r\n",
      "terminado                    0.13.3\r\n",
      "testpath                     0.6.0\r\n",
      "textblob                     0.17.1\r\n",
      "tf-estimator-nightly         2.8.0.dev2021122109\r\n",
      "tfrecord                     1.14.1\r\n",
      "thinc                        7.4.6\r\n",
      "threadpoolctl                3.1.0\r\n",
      "tifffile                     2022.2.9\r\n",
      "tokenizers                   0.13.2\r\n",
      "toml                         0.10.2\r\n",
      "toolz                        0.12.0\r\n",
      "torch                        1.13.0+cu116\r\n",
      "torchaudio                   0.13.0+cu116\r\n",
      "torchmetrics                 0.8.2\r\n",
      "torchvision                  0.14.0+cu116\r\n",
      "tornado                      6.1\r\n",
      "tqdm                         4.63.0\r\n",
      "traitlets                    5.1.1\r\n",
      "transformers                 4.26.0.dev0\r\n",
      "typing                       3.7.4.3\r\n",
      "typing_extensions            4.1.1\r\n",
      "umap-learn                   0.5.3\r\n",
      "urllib3                      1.26.13\r\n",
      "utility                      1.0\r\n",
      "varname                      0.8.3\r\n",
      "visions                      0.7.5\r\n",
      "wadllib                      1.3.3\r\n",
      "wasabi                       0.10.1\r\n",
      "wcwidth                      0.2.5\r\n",
      "webargs                      8.2.0\r\n",
      "webencodings                 0.5.1\r\n",
      "websocket-client             1.3.1\r\n",
      "Werkzeug                     2.2.2\r\n",
      "wheel                        0.37.1\r\n",
      "whisper                      1.1.10\r\n",
      "widgetsnbextension           3.5.2\r\n",
      "wikiextractor                3.0.6\r\n",
      "wordcloud                    1.8.2.2\r\n",
      "wrapt                        1.13.3\r\n",
      "xgboost                      1.6.1\r\n",
      "xlrd                         2.0.1\r\n",
      "xxhash                       3.1.0\r\n",
      "yarl                         1.7.2\r\n",
      "yellowbrick                  1.3.post1\r\n",
      "zipp                         3.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c056ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 사용\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b3f787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /akidev/NLP/Labs/.cache/kobert_v1.zip\n",
      "using cached model. /akidev/NLP/Labs/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c59831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sen = pd.read_csv('단발성 대화 정리.csv')\n",
    "long_sen = pd.read_csv('연속적 대화 정리.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e908152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화</th>\n",
       "      <th>감정</th>\n",
       "      <th>감정.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22901</th>\n",
       "      <td>포켓몬 자체가 드물게 나옴.</td>\n",
       "      <td>중립</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20024</th>\n",
       "      <td>내일이 마지막회라니...</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>기저귀에 오줌 많이 싸면 원래 옷이 축축해 지나요?</td>\n",
       "      <td>놀람</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 발화  감정  감정.1\n",
       "22901               포켓몬 자체가 드물게 나옴.  중립     2\n",
       "20024                 내일이 마지막회라니...  슬픔     3\n",
       "7541   기저귀에 오줌 많이 싸면 원래 옷이 축축해 지나요?  놀람     7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_sen.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb69a416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화</th>\n",
       "      <th>감정_str</th>\n",
       "      <th>감정_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>아 왜</td>\n",
       "      <td>분노</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44194</th>\n",
       "      <td>그 녀석 혼자서 내 아내를 온통 독차지하고 있으니까 그렇지, 이대로 일주일만 더 버...</td>\n",
       "      <td>중립</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43905</th>\n",
       "      <td>무슨 말?</td>\n",
       "      <td>중립</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      발화 감정_str  감정_int\n",
       "693                                                  아 왜     분노       6\n",
       "44194  그 녀석 혼자서 내 아내를 온통 독차지하고 있으니까 그렇지, 이대로 일주일만 더 버...     중립       2\n",
       "43905                                             무슨 말?      중립       2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_sen.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb559f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sen = short_sen.rename(columns={\"발화\":'sentence','감정':'str','감정.1':'Emotion'})\n",
    "long_sen = long_sen.rename(columns={\"발화\":'sentence','감정_str':'str','감정_int':'Emotion'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ef5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.concat([short_sen, long_sen], axis=0)\n",
    "total_data = total_data.replace({'Emotion' : 7}, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99be20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = list(total_data['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892a581a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 6, 3, 2, 1, 5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02b53218",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {}\n",
    "for i in sorted(emotion_list):\n",
    "    temp = total_data[total_data['Emotion']==i].sample(1)['str'].values[0]\n",
    "#     print(temp)\n",
    "    emotion_dict[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "136e8bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '놀람', 1: '행복', 2: '중립', 3: '슬픔', 4: '공포', 5: '혐오', 6: '분노'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99e63ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for q, label in zip(total_data['sentence'], total_data['Emotion'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e769ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['언니 동생으로 부르는게 맞는 일인가요..??', '4'],\n",
       " ['그냥 내 느낌일뿐겠지?', '4'],\n",
       " ['아직너무초기라서 그런거죠?', '4']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db37ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train & test 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19dcbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70665\n",
      "23556\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c090612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4240d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 128\n",
    "batch_size = 13\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1993a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /akidev/NLP/Labs/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b677dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22625bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 4209, 2707, 5512, 6999,  517,   54,    3,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1], dtype=int32),\n",
       " array(8, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83dbd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d645513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=7,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8255050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dd27c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1988477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d057103",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d18ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a037082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ac5a1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f839639fee0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f108770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad33620d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c9d6138a2f46e1a9082de856e17c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.2268226146698 train acc 0.0\n",
      "epoch 1 batch id 201 loss 1.832778811454773 train acc 0.13892078071182526\n",
      "epoch 1 batch id 401 loss 1.5435513257980347 train acc 0.3211202762324956\n",
      "epoch 1 batch id 601 loss 1.0040916204452515 train acc 0.39075899142454823\n",
      "epoch 1 batch id 801 loss 2.0267934799194336 train acc 0.4243733794295591\n",
      "epoch 1 batch id 1001 loss 1.145259976387024 train acc 0.4443249058633677\n",
      "epoch 1 batch id 1201 loss 1.1498080492019653 train acc 0.4639723307500171\n",
      "epoch 1 batch id 1401 loss 1.7771250009536743 train acc 0.47861417668698303\n",
      "epoch 1 batch id 1601 loss 1.12114417552948 train acc 0.4904146446932223\n",
      "epoch 1 batch id 1801 loss 1.1212904453277588 train acc 0.5019433647973368\n",
      "epoch 1 batch id 2001 loss 0.9531456828117371 train acc 0.5129358397724241\n",
      "epoch 1 batch id 2201 loss 1.041943073272705 train acc 0.5214413029042727\n",
      "epoch 1 batch id 2401 loss 0.8145015239715576 train acc 0.5295229551789263\n",
      "epoch 1 batch id 2601 loss 0.724514901638031 train acc 0.5353562239375305\n",
      "epoch 1 batch id 2801 loss 1.0626660585403442 train acc 0.541729602065185\n",
      "epoch 1 batch id 3001 loss 0.71701979637146 train acc 0.5483300438315288\n",
      "epoch 1 batch id 3201 loss 0.5762401819229126 train acc 0.5518467786508867\n",
      "epoch 1 batch id 3401 loss 0.5726071000099182 train acc 0.5574604754257623\n",
      "epoch 1 batch id 3601 loss 0.8064189553260803 train acc 0.5609766517847417\n",
      "epoch 1 batch id 3801 loss 1.286793828010559 train acc 0.565498957764134\n",
      "epoch 1 batch id 4001 loss 0.45879971981048584 train acc 0.5690884971064779\n",
      "epoch 1 batch id 4201 loss 0.4880000650882721 train acc 0.5726475381319339\n",
      "epoch 1 batch id 4401 loss 1.003221035003662 train acc 0.5749043049656439\n",
      "epoch 1 batch id 4601 loss 1.1854500770568848 train acc 0.5780683129085564\n",
      "epoch 1 batch id 4801 loss 1.1130677461624146 train acc 0.5803598609264025\n",
      "epoch 1 batch id 5001 loss 0.8204514384269714 train acc 0.5827296079245639\n",
      "epoch 1 batch id 5201 loss 0.4498712122440338 train acc 0.5847987812994505\n",
      "epoch 1 batch id 5401 loss 0.7218742370605469 train acc 0.5865580448065153\n",
      "epoch 1 train acc 0.5867365710080923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042392b93d43467a8c78dd340282e35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6594498217014706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90e915f5b8548d0aa48f13ece08f867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.5572728514671326 train acc 0.8461538461538461\n",
      "epoch 2 batch id 201 loss 0.8416933417320251 train acc 0.6371986222732486\n",
      "epoch 2 batch id 401 loss 1.2523777484893799 train acc 0.6389794743909444\n",
      "epoch 2 batch id 601 loss 0.5750845670700073 train acc 0.6434148214514284\n",
      "epoch 2 batch id 801 loss 1.2003669738769531 train acc 0.6478440411024702\n",
      "epoch 2 batch id 1001 loss 1.191827654838562 train acc 0.6475063398140336\n",
      "epoch 2 batch id 1201 loss 1.1674364805221558 train acc 0.6493306859668242\n",
      "epoch 2 batch id 1401 loss 1.4787191152572632 train acc 0.650908691593918\n",
      "epoch 2 batch id 1601 loss 1.230195164680481 train acc 0.6496420506414274\n",
      "epoch 2 batch id 1801 loss 1.0963774919509888 train acc 0.6495963780805488\n",
      "epoch 2 batch id 2001 loss 1.136215329170227 train acc 0.6493676238803585\n",
      "epoch 2 batch id 2201 loss 1.1197589635849 train acc 0.6496697305420482\n",
      "epoch 2 batch id 2401 loss 0.7776736617088318 train acc 0.6503059622592985\n",
      "epoch 2 batch id 2601 loss 0.532563328742981 train acc 0.6513175405908779\n",
      "epoch 2 batch id 2801 loss 1.0138691663742065 train acc 0.6524592865185292\n",
      "epoch 2 batch id 3001 loss 0.6852203607559204 train acc 0.6541152949016757\n",
      "epoch 2 batch id 3201 loss 0.4451516568660736 train acc 0.6538581693220634\n",
      "epoch 2 batch id 3401 loss 0.40430518984794617 train acc 0.6557799742157099\n",
      "epoch 2 batch id 3601 loss 0.781889796257019 train acc 0.657210603892066\n",
      "epoch 2 batch id 3801 loss 1.4403111934661865 train acc 0.659118045858365\n",
      "epoch 2 batch id 4001 loss 0.31251177191734314 train acc 0.6597581373887192\n",
      "epoch 2 batch id 4201 loss 0.3778609037399292 train acc 0.6611795726292198\n",
      "epoch 2 batch id 4401 loss 0.7397820353507996 train acc 0.6622445947599269\n",
      "epoch 2 batch id 4601 loss 1.196681261062622 train acc 0.6636851520572417\n",
      "epoch 2 batch id 4801 loss 0.8921485543251038 train acc 0.6639802605226456\n",
      "epoch 2 batch id 5001 loss 0.6515796184539795 train acc 0.6653746173842153\n",
      "epoch 2 batch id 5201 loss 0.42780807614326477 train acc 0.666676526703446\n",
      "epoch 2 batch id 5401 loss 0.4304428696632385 train acc 0.6676826228761086\n",
      "epoch 2 train acc 0.6679133412577154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f3e20be6794022ac321e3489b09485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.6722278824927759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7b2af68d074e48be566bac2acb9b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.37439611554145813 train acc 0.9230769230769231\n",
      "epoch 3 batch id 201 loss 0.7499032616615295 train acc 0.6930730960581698\n",
      "epoch 3 batch id 401 loss 1.1269079446792603 train acc 0.6932668329177046\n",
      "epoch 3 batch id 601 loss 0.5346275568008423 train acc 0.6938435940099836\n",
      "epoch 3 batch id 801 loss 1.347634196281433 train acc 0.6957649092480562\n",
      "epoch 3 batch id 1001 loss 0.9434661269187927 train acc 0.6979943133789288\n",
      "epoch 3 batch id 1201 loss 0.9913270473480225 train acc 0.6994812015628\n",
      "epoch 3 batch id 1401 loss 1.3137242794036865 train acc 0.7001592269258218\n",
      "epoch 3 batch id 1601 loss 0.8159326910972595 train acc 0.699995195310618\n",
      "epoch 3 batch id 1801 loss 1.2561123371124268 train acc 0.7008926664673367\n",
      "epoch 3 batch id 2001 loss 0.7930331826210022 train acc 0.7025717910275483\n",
      "epoch 3 batch id 2201 loss 1.0395523309707642 train acc 0.7029671827490818\n",
      "epoch 3 batch id 2401 loss 0.8154348731040955 train acc 0.7031365136321192\n",
      "epoch 3 batch id 2601 loss 0.3556278944015503 train acc 0.7034572501700294\n",
      "epoch 3 batch id 2801 loss 0.9359649419784546 train acc 0.7045835278609031\n",
      "epoch 3 batch id 3001 loss 0.5134400725364685 train acc 0.7059185399738308\n",
      "epoch 3 batch id 3201 loss 0.2606413662433624 train acc 0.7061975824862223\n",
      "epoch 3 batch id 3401 loss 0.3772607147693634 train acc 0.7085472598556833\n",
      "epoch 3 batch id 3601 loss 0.4548470079898834 train acc 0.7099737252472494\n",
      "epoch 3 batch id 3801 loss 1.382344126701355 train acc 0.7117762532127093\n",
      "epoch 3 batch id 4001 loss 0.15587052702903748 train acc 0.7126103089612159\n",
      "epoch 3 batch id 4201 loss 0.26214930415153503 train acc 0.7146466958416472\n",
      "epoch 3 batch id 4401 loss 0.8469117283821106 train acc 0.7161659063499553\n",
      "epoch 3 batch id 4601 loss 0.8453823924064636 train acc 0.7175697590824758\n",
      "epoch 3 batch id 4801 loss 0.6990739107131958 train acc 0.7182478009389109\n",
      "epoch 3 batch id 5001 loss 0.23422227799892426 train acc 0.7189638995277926\n",
      "epoch 3 batch id 5201 loss 0.3313572108745575 train acc 0.7204383772351552\n",
      "epoch 3 batch id 5401 loss 0.3579230308532715 train acc 0.7214760799282286\n",
      "epoch 3 train acc 0.7217425142921937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553cf3d7739b42d9ac37af9b4b3b8127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6754542367125066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e421edf6a7c4268a4bbef6affd449c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.14185276627540588 train acc 1.0\n",
      "epoch 4 batch id 201 loss 0.6270924210548401 train acc 0.7447378492154607\n",
      "epoch 4 batch id 401 loss 0.8451785445213318 train acc 0.7479378476884708\n",
      "epoch 4 batch id 601 loss 0.313155859708786 train acc 0.7491360552924622\n",
      "epoch 4 batch id 801 loss 0.9117092490196228 train acc 0.7479112647651975\n",
      "epoch 4 batch id 1001 loss 1.1719179153442383 train acc 0.7481749020210543\n",
      "epoch 4 batch id 1201 loss 0.710386335849762 train acc 0.7501441106769968\n",
      "epoch 4 batch id 1401 loss 1.0648499727249146 train acc 0.7522648657552248\n",
      "epoch 4 batch id 1601 loss 0.7846397757530212 train acc 0.7527987315619936\n",
      "epoch 4 batch id 1801 loss 0.8981069326400757 train acc 0.7540682526801197\n",
      "epoch 4 batch id 2001 loss 0.5605455636978149 train acc 0.7542767077999277\n",
      "epoch 4 batch id 2201 loss 0.8911842107772827 train acc 0.7553210079334355\n",
      "epoch 4 batch id 2401 loss 0.3589771091938019 train acc 0.7561272546695051\n",
      "epoch 4 batch id 2601 loss 0.2687629163265228 train acc 0.7558927039895643\n",
      "epoch 4 batch id 2801 loss 0.6559054851531982 train acc 0.7570098591162261\n",
      "epoch 4 batch id 3001 loss 0.4497356116771698 train acc 0.7591315715274222\n",
      "epoch 4 batch id 3201 loss 0.15309356153011322 train acc 0.7587532742171789\n",
      "epoch 4 batch id 3401 loss 0.2546691596508026 train acc 0.760703865378954\n",
      "epoch 4 batch id 3601 loss 0.5298244953155518 train acc 0.7617542135731495\n",
      "epoch 4 batch id 3801 loss 1.299008846282959 train acc 0.7630178293161726\n",
      "epoch 4 batch id 4001 loss 0.14408987760543823 train acc 0.7645780862476731\n",
      "epoch 4 batch id 4201 loss 0.1125369593501091 train acc 0.7660813359456609\n",
      "epoch 4 batch id 4401 loss 0.5437805652618408 train acc 0.7674654361771024\n",
      "epoch 4 batch id 4601 loss 0.9822348356246948 train acc 0.7687793623459903\n",
      "epoch 4 batch id 4801 loss 0.4457542300224304 train acc 0.7689263454729122\n",
      "epoch 4 batch id 5001 loss 0.09688328951597214 train acc 0.769938320028318\n",
      "epoch 4 batch id 5201 loss 0.20587195456027985 train acc 0.7714492775058229\n",
      "epoch 4 batch id 5401 loss 0.10435844212770462 train acc 0.7727201515389092\n",
      "epoch 4 train acc 0.7727500424520498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4baff40dec5f4ccb9f46b129d8ef5f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.6673034471047646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f91f7a0af0f41fdbce13028d343fb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.0698346421122551 train acc 1.0\n",
      "epoch 5 batch id 201 loss 0.636987566947937 train acc 0.7895139686184456\n",
      "epoch 5 batch id 401 loss 0.7666612267494202 train acc 0.7889890657970456\n",
      "epoch 5 batch id 601 loss 0.4737984538078308 train acc 0.7944451555100484\n",
      "epoch 5 batch id 801 loss 0.7997013330459595 train acc 0.7924709497743195\n",
      "epoch 5 batch id 1001 loss 0.961467444896698 train acc 0.7945131791285605\n",
      "epoch 5 batch id 1201 loss 0.22849451005458832 train acc 0.7987574457183061\n",
      "epoch 5 batch id 1401 loss 1.7981950044631958 train acc 0.7988250150990958\n",
      "epoch 5 batch id 1601 loss 0.44618895649909973 train acc 0.8001729688175526\n",
      "epoch 5 batch id 1801 loss 0.5804777145385742 train acc 0.8019476359287413\n",
      "epoch 5 batch id 2001 loss 0.447909951210022 train acc 0.8023296044285362\n",
      "epoch 5 batch id 2201 loss 0.5471876859664917 train acc 0.8028169014084297\n",
      "epoch 5 batch id 2401 loss 0.9827404618263245 train acc 0.8035433953801073\n",
      "epoch 5 batch id 2601 loss 0.10814160853624344 train acc 0.8038919942033904\n",
      "epoch 5 batch id 2801 loss 0.6477756500244141 train acc 0.8055914096613684\n",
      "epoch 5 batch id 3001 loss 0.7883103489875793 train acc 0.8073975341552717\n",
      "epoch 5 batch id 3201 loss 0.1007738783955574 train acc 0.8076562612645033\n",
      "epoch 5 batch id 3401 loss 0.16358090937137604 train acc 0.8093320968945773\n",
      "epoch 5 batch id 3601 loss 0.3392431139945984 train acc 0.8100527631213583\n",
      "epoch 5 batch id 3801 loss 1.028581976890564 train acc 0.8118106571145317\n",
      "epoch 5 batch id 4001 loss 0.03791861608624458 train acc 0.8132005460173521\n",
      "epoch 5 batch id 4201 loss 0.0769391879439354 train acc 0.8144214747404606\n",
      "epoch 5 batch id 4401 loss 0.6079586148262024 train acc 0.8155838707986116\n",
      "epoch 5 batch id 4601 loss 0.6600967049598694 train acc 0.8172470867537355\n",
      "epoch 5 batch id 4801 loss 0.503800094127655 train acc 0.8179385704901443\n",
      "epoch 5 batch id 5001 loss 0.03497742488980293 train acc 0.8191284819959326\n",
      "epoch 5 batch id 5201 loss 0.2052757740020752 train acc 0.8200937689497865\n",
      "epoch 5 batch id 5401 loss 0.047470398247241974 train acc 0.8206599917394519\n",
      "epoch 5 train acc 0.8208481915435867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e646dfc9115c4a968b2c355dcbb1e951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.6614026150449925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cc7ea953fa44979f50a4be5ed41953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.034813567996025085 train acc 1.0\n",
      "epoch 6 batch id 201 loss 0.20407380163669586 train acc 0.8430922311519318\n",
      "epoch 6 batch id 401 loss 0.6599572896957397 train acc 0.8455783617878384\n",
      "epoch 6 batch id 601 loss 0.24234384298324585 train acc 0.846665813387944\n",
      "epoch 6 batch id 801 loss 0.6849849224090576 train acc 0.845673677134347\n",
      "epoch 6 batch id 1001 loss 0.5142568349838257 train acc 0.8482286943825338\n",
      "epoch 6 batch id 1201 loss 0.2356029599905014 train acc 0.8505732402485019\n",
      "epoch 6 batch id 1401 loss 1.242975115776062 train acc 0.851809147312347\n",
      "epoch 6 batch id 1601 loss 0.14306987822055817 train acc 0.8525921299187863\n",
      "epoch 6 batch id 1801 loss 0.787255585193634 train acc 0.8528595224874905\n",
      "epoch 6 batch id 2001 loss 0.6171503663063049 train acc 0.8534963287586796\n",
      "epoch 6 batch id 2201 loss 0.49298661947250366 train acc 0.8537028623352828\n",
      "epoch 6 batch id 2401 loss 0.6400877237319946 train acc 0.8544837087110939\n",
      "epoch 6 batch id 2601 loss 0.1513262391090393 train acc 0.8544346848844981\n",
      "epoch 6 batch id 2801 loss 0.6472948789596558 train acc 0.8561777387196804\n",
      "epoch 6 batch id 3001 loss 0.08645994961261749 train acc 0.8573039755978765\n",
      "epoch 6 batch id 3201 loss 0.04764603078365326 train acc 0.8579290125681917\n",
      "epoch 6 batch id 3401 loss 0.048151977360248566 train acc 0.8592495419899208\n",
      "epoch 6 batch id 3601 loss 0.30237454175949097 train acc 0.8600388780894322\n",
      "epoch 6 batch id 3801 loss 1.133202314376831 train acc 0.8615748891992148\n",
      "epoch 6 batch id 4001 loss 0.0142783522605896 train acc 0.8629765635514385\n",
      "epoch 6 batch id 4201 loss 0.017477206885814667 train acc 0.8639151850292279\n",
      "epoch 6 batch id 4401 loss 0.49810612201690674 train acc 0.8648209323056203\n",
      "epoch 6 batch id 4601 loss 0.7038371562957764 train acc 0.865948873990632\n",
      "epoch 6 batch id 4801 loss 0.1576158106327057 train acc 0.8664220595068646\n",
      "epoch 6 batch id 5001 loss 0.2832314074039459 train acc 0.8669804500638688\n",
      "epoch 6 batch id 5201 loss 0.03618090599775314 train acc 0.8681170780767405\n",
      "epoch 6 batch id 5401 loss 0.02847466617822647 train acc 0.8687992252147457\n",
      "epoch 6 train acc 0.8690736967227457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ca0dc1750046a18c10b46fa2bfb49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.6588554932925739\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8211493e5c4a484d91fa008c269efcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.015683487057685852 train acc 1.0\n",
      "epoch 7 batch id 201 loss 0.2559671998023987 train acc 0.8836586299272865\n",
      "epoch 7 batch id 401 loss 0.6248840093612671 train acc 0.8868214080184155\n",
      "epoch 7 batch id 601 loss 0.1620505452156067 train acc 0.8895430692435672\n",
      "epoch 7 batch id 801 loss 0.28130489587783813 train acc 0.889465091712277\n",
      "epoch 7 batch id 1001 loss 0.42606988549232483 train acc 0.8908015061861136\n",
      "epoch 7 batch id 1201 loss 0.5246597528457642 train acc 0.8923333119835933\n",
      "epoch 7 batch id 1401 loss 1.114979863166809 train acc 0.8937572063910275\n",
      "epoch 7 batch id 1601 loss 0.112274169921875 train acc 0.8940565992408458\n",
      "epoch 7 batch id 1801 loss 1.2201327085494995 train acc 0.8946311878016343\n",
      "epoch 7 batch id 2001 loss 0.3745410144329071 train acc 0.8963210702340988\n",
      "epoch 7 batch id 2201 loss 0.5857125520706177 train acc 0.8962010275049646\n",
      "epoch 7 batch id 2401 loss 0.22825218737125397 train acc 0.897254349149381\n",
      "epoch 7 batch id 2601 loss 0.12025944888591766 train acc 0.8973175997397402\n",
      "epoch 7 batch id 2801 loss 0.14171987771987915 train acc 0.8989097300414698\n",
      "epoch 7 batch id 3001 loss 0.08942026644945145 train acc 0.8994181426703981\n",
      "epoch 7 batch id 3201 loss 0.06857364624738693 train acc 0.8997909307187774\n",
      "epoch 7 batch id 3401 loss 0.26858779788017273 train acc 0.9007079365797547\n",
      "epoch 7 batch id 3601 loss 0.3130955100059509 train acc 0.9012881037318896\n",
      "epoch 7 batch id 3801 loss 0.9386571645736694 train acc 0.902252443688931\n",
      "epoch 7 batch id 4001 loss 0.00459668505936861 train acc 0.902812758348901\n",
      "epoch 7 batch id 4201 loss 0.010759470984339714 train acc 0.9032098584586381\n",
      "epoch 7 batch id 4401 loss 0.2674930691719055 train acc 0.9038505234824569\n",
      "epoch 7 batch id 4601 loss 0.4887889325618744 train acc 0.9046193971210615\n",
      "epoch 7 batch id 4801 loss 0.4088341295719147 train acc 0.9045551407559701\n",
      "epoch 7 batch id 5001 loss 0.006781016010791063 train acc 0.904972851583571\n",
      "epoch 7 batch id 5201 loss 0.24515829980373383 train acc 0.9054915474835173\n",
      "epoch 7 batch id 5401 loss 0.018114907667040825 train acc 0.9062139489838545\n",
      "epoch 7 train acc 0.906421576951427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5be9b04b3ed439097849817bba18e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.6668789268126947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffe87511eff4ad1abba007b31d34a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.008779395371675491 train acc 1.0\n",
      "epoch 8 batch id 201 loss 0.023065807297825813 train acc 0.9161882893226174\n",
      "epoch 8 batch id 401 loss 0.4243377149105072 train acc 0.9178975637828507\n",
      "epoch 8 batch id 601 loss 0.15985694527626038 train acc 0.9200051196723389\n",
      "epoch 8 batch id 801 loss 0.5021116137504578 train acc 0.9193316047248578\n",
      "epoch 8 batch id 1001 loss 0.5799574255943298 train acc 0.9212326135402986\n",
      "epoch 8 batch id 1201 loss 0.024830251932144165 train acc 0.9229488246973593\n",
      "epoch 8 batch id 1401 loss 1.3503690958023071 train acc 0.9233514522593659\n",
      "epoch 8 batch id 1601 loss 0.011378458701074123 train acc 0.9240378609522792\n",
      "epoch 8 batch id 1801 loss 0.7361713647842407 train acc 0.9241447059325906\n",
      "epoch 8 batch id 2001 loss 0.3519093096256256 train acc 0.9248452696728445\n",
      "epoch 8 batch id 2201 loss 0.45627591013908386 train acc 0.9247544822283458\n",
      "epoch 8 batch id 2401 loss 0.02483535185456276 train acc 0.9253195783807976\n",
      "epoch 8 batch id 2601 loss 0.008415916934609413 train acc 0.9256794723922763\n",
      "epoch 8 batch id 2801 loss 0.033286988735198975 train acc 0.9268393156290397\n",
      "epoch 8 batch id 3001 loss 0.010498515330255032 train acc 0.9273319150027025\n",
      "epoch 8 batch id 3201 loss 0.3545975685119629 train acc 0.9278350515464061\n",
      "epoch 8 batch id 3401 loss 0.007625505328178406 train acc 0.9286861330378114\n",
      "epoch 8 batch id 3601 loss 0.2571735382080078 train acc 0.9287804669643254\n",
      "epoch 8 batch id 3801 loss 0.5987856388092041 train acc 0.9297148523668092\n",
      "epoch 8 batch id 4001 loss 0.00739348866045475 train acc 0.9301905292907797\n",
      "epoch 8 batch id 4201 loss 0.004324224311858416 train acc 0.9307490890447606\n",
      "epoch 8 batch id 4401 loss 0.40860825777053833 train acc 0.9311520109066417\n",
      "epoch 8 batch id 4601 loss 0.5104595422744751 train acc 0.9318542791701132\n",
      "epoch 8 batch id 4801 loss 0.015833033248782158 train acc 0.9317930559338967\n",
      "epoch 8 batch id 5001 loss 0.0062974621541798115 train acc 0.9320289788196605\n",
      "epoch 8 batch id 5201 loss 0.00431875791400671 train acc 0.9322467572804479\n",
      "epoch 8 batch id 5401 loss 0.012535754591226578 train acc 0.9323344679760617\n",
      "epoch 8 train acc 0.9325012735609224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6d7da699a84c3eb5317e1c542648df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.6657327220241062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da31437ebf444168f26bb8b24f59e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 0.012440106831490993 train acc 1.0\n",
      "epoch 9 batch id 201 loss 0.007677721790969372 train acc 0.9376195943360124\n",
      "epoch 9 batch id 401 loss 0.1646171510219574 train acc 0.9401496259351619\n",
      "epoch 9 batch id 601 loss 0.18084441125392914 train acc 0.941763727121462\n",
      "epoch 9 batch id 801 loss 0.06429542601108551 train acc 0.9405550753865315\n",
      "epoch 9 batch id 1001 loss 0.3541695475578308 train acc 0.9418274033658589\n",
      "epoch 9 batch id 1201 loss 0.1947484016418457 train acc 0.9435726638057963\n",
      "epoch 9 batch id 1401 loss 0.4437467157840729 train acc 0.943446988414861\n",
      "epoch 9 batch id 1601 loss 0.005074365995824337 train acc 0.9444577908038166\n",
      "epoch 9 batch id 1801 loss 0.6592716574668884 train acc 0.9442617349335753\n",
      "epoch 9 batch id 2001 loss 0.15913823246955872 train acc 0.9449121593049541\n",
      "epoch 9 batch id 2201 loss 0.4028114974498749 train acc 0.9449201411945541\n",
      "epoch 9 batch id 2401 loss 0.011677619069814682 train acc 0.9451510588536809\n",
      "epoch 9 batch id 2601 loss 0.0034752637147903442 train acc 0.9448732735930007\n",
      "epoch 9 batch id 2801 loss 0.05884847417473793 train acc 0.9454865020734433\n",
      "epoch 9 batch id 3001 loss 0.01901460997760296 train acc 0.9455309768538812\n",
      "epoch 9 batch id 3201 loss 0.004138793796300888 train acc 0.9460024511571057\n",
      "epoch 9 batch id 3401 loss 0.010124205611646175 train acc 0.94662203424334\n",
      "epoch 9 batch id 3601 loss 0.21510770916938782 train acc 0.9466387541922316\n",
      "epoch 9 batch id 3801 loss 0.5456879734992981 train acc 0.947139416752699\n",
      "epoch 9 batch id 4001 loss 0.0019891629926860332 train acc 0.9476092515332944\n",
      "epoch 9 batch id 4201 loss 0.0030423442367464304 train acc 0.9475948949883987\n",
      "epoch 9 batch id 4401 loss 0.24194583296775818 train acc 0.9479663712792828\n",
      "epoch 9 batch id 4601 loss 0.487218976020813 train acc 0.9484894588133322\n",
      "epoch 9 batch id 4801 loss 0.006673956755548716 train acc 0.9482960280711088\n",
      "epoch 9 batch id 5001 loss 0.0024329768493771553 train acc 0.9486718041007544\n",
      "epoch 9 batch id 5201 loss 0.002379209268838167 train acc 0.9486489284605417\n",
      "epoch 9 batch id 5401 loss 0.013445998542010784 train acc 0.9488698674035128\n",
      "epoch 9 train acc 0.9489443595404268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43afa5f75a9467f95e1f5fe35e7c989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 test acc 0.6647138733231382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09b84ad1707446e9c6310c197626e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 0.0030948324128985405 train acc 1.0\n",
      "epoch 10 batch id 201 loss 0.007112410850822926 train acc 0.9468044393417533\n",
      "epoch 10 batch id 601 loss 0.12526190280914307 train acc 0.9495712274414415\n",
      "epoch 10 batch id 801 loss 0.21432074904441833 train acc 0.9506386247959241\n",
      "epoch 10 batch id 1001 loss 0.2735116183757782 train acc 0.9529701068162557\n",
      "epoch 10 batch id 1201 loss 0.005040218587964773 train acc 0.9549734195862367\n",
      "epoch 10 batch id 1401 loss 0.414859801530838 train acc 0.9550870257508312\n",
      "epoch 10 batch id 1601 loss 0.004734240937978029 train acc 0.955796857733141\n",
      "epoch 10 batch id 1801 loss 0.7886164784431458 train acc 0.9552812540041787\n",
      "epoch 10 batch id 2001 loss 0.00708001758903265 train acc 0.9553684696113409\n",
      "epoch 10 batch id 2201 loss 0.4687683880329132 train acc 0.9549854961031637\n",
      "epoch 10 batch id 2401 loss 0.004826681688427925 train acc 0.9551148559894902\n",
      "epoch 10 batch id 2601 loss 0.0023312384728342295 train acc 0.9549285777659521\n",
      "epoch 10 batch id 2801 loss 0.008425723761320114 train acc 0.9553730810424926\n",
      "epoch 10 batch id 3001 loss 0.013595147989690304 train acc 0.9551175249275996\n",
      "epoch 10 batch id 3201 loss 0.0029714491683989763 train acc 0.955158243818053\n",
      "epoch 10 batch id 3401 loss 0.012010803446173668 train acc 0.9556012937371542\n",
      "epoch 10 batch id 3601 loss 0.22345010936260223 train acc 0.9555892593937787\n",
      "epoch 10 batch id 3801 loss 1.1106219291687012 train acc 0.9558011049723988\n",
      "epoch 10 batch id 4001 loss 0.0023875900078564882 train acc 0.9558956414742712\n",
      "epoch 10 batch id 4201 loss 0.003073382657021284 train acc 0.9556149634702625\n",
      "epoch 10 batch id 4401 loss 0.20867377519607544 train acc 0.9559540663835429\n",
      "epoch 10 batch id 4601 loss 0.4357883036136627 train acc 0.9562804072693535\n",
      "epoch 10 batch id 4801 loss 0.0060738143511116505 train acc 0.9560027558361573\n",
      "epoch 10 batch id 5001 loss 0.0021642325446009636 train acc 0.9560703243966949\n",
      "epoch 10 batch id 5201 loss 0.002381931757554412 train acc 0.9561474864301623\n",
      "epoch 10 batch id 5401 loss 0.006831685081124306 train acc 0.9561762066854107\n",
      "epoch 10 train acc 0.9562319578876263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fda7ebd09f4ee3afc301e5cf66908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 test acc 0.6614875191034064\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8509812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /akidev/NLP/Labs/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42a6821c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef predict(predict_sentence):\\n\\n    data = [predict_sentence, \\'0\\']\\n    dataset_another = [data]\\n\\n    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\\n    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\\n    \\n    model.eval()\\n\\n    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\\n        token_ids = token_ids.long().to(device)\\n        segment_ids = segment_ids.long().to(device)\\n\\n        valid_length= valid_length\\n        label = label.long().to(device)\\n\\n        out = model(token_ids, valid_length, segment_ids)\\n\\n\\n        test_eval=[]\\n        for i in out:\\n            logits=i\\n            logits = logits.detach().cpu().numpy()\\n            print(np.argmax(logits))\\n            if np.argmax(logits) == 1:\\n                test_eval.append(\"행복이\")\\n            elif np.argmax(logits) == 2:\\n                test_eval.append(\"중립이\")\\n            elif np.argmax(logits) == 3:\\n                test_eval.append(\"슬픔이\")\\n            elif np.argmax(logits) == 4:\\n                test_eval.append(\"공포가\")\\n            elif np.argmax(logits) == 5:\\n                test_eval.append(\"혐오가\")\\n            elif np.argmax(logits) == 6:\\n                test_eval.append(\"분노가\")\\n            elif np.argmax(logits) == 7:\\n                test_eval.append(\"놀람이\")\\n\\n        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")\\n        '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            print(np.argmax(logits))\n",
    "            if np.argmax(logits) == 1:\n",
    "                test_eval.append(\"행복이\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                test_eval.append(\"중립이\")\n",
    "            elif np.argmax(logits) == 3:\n",
    "                test_eval.append(\"슬픔이\")\n",
    "            elif np.argmax(logits) == 4:\n",
    "                test_eval.append(\"공포가\")\n",
    "            elif np.argmax(logits) == 5:\n",
    "                test_eval.append(\"혐오가\")\n",
    "            elif np.argmax(logits) == 6:\n",
    "                test_eval.append(\"분노가\")\n",
    "            elif np.argmax(logits) == 7:\n",
    "                test_eval.append(\"놀람이\")\n",
    "\n",
    "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d0043a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하고싶은 말을 입력해주세요 : 디자인은그럭저럭 괜찮은데 배송이좀문제네요 조립은 쉬웠는데 육각렌치가 너무 힘도없고 채결하는게좀힘들었네요\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mpredict\u001b[49m(sentence)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "#질문 무한반복하기! 0 입력시 종료\n",
    "end = 1\n",
    "while end == 1 :\n",
    "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
    "    if sentence == 0:\n",
    "        break\n",
    "    predict(sentence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31841ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "#         test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            emotion = emotion_dict[np.argmax(logits)]\n",
    "            \n",
    "\n",
    "        print(f\">> 입력하신 내용의 감정은 {emotion}입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "593dc4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하고싶은 말을 입력해주세요 : 디자인은그럭저럭 괜찮은데 배송이좀문제네요 조립은 쉬웠는데 육각렌치가 너무 힘도없고 채결하는게좀힘들었네요\n",
      ">> 입력하신 내용의 감정은 슬픔입니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 다 좋은데 생산을 잘 못 했는지, 배송 문제인지 이음부분이 찌그러져있어서 결합이 안 되길래 함마로 그냥 내리 꽂아서 폈어요 역시 맞아야 해요.\n",
      ">> 입력하신 내용의 감정은 중립입니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 특히 여성분 혼자라면!! 저는 전동드릴도있고 원래 조립가구를 좋아하는 편이라 총 20분 내외 정도 걸린것 같습니다. 그래도 조립하고나니 튼튼한 느낌이들어서 저는 마음에 들어욥!\n",
      ">> 입력하신 내용의 감정은 행복입니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 디자인이랑 튼튼하긴 튼튼한데 상판도 까져서 왔고 밑에 프레임은 페인트도 벗겨져있더라고요 교환하기엔 책상이 진짜 급해서 안하는데 배송 기사님은 말도 없이 1층 로비에 두고가시고… 디자인 빼면 최악이네요..^^\n",
      ">> 입력하신 내용의 감정은 분노입니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 두닷 콰트로 에어가 모서리에 곡선이 유니크하고 예뻐서 선택했어요 마감은 말할것도 없고 조립도 어렵지 않았어요! 깨끗하고 튼튼하고 아주 맘에 듭니다 군더더기 없는 디자인 👍\n",
      ">> 입력하신 내용의 감정은 행복입니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : quit\n",
      "감정 분석을 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
    "    if sentence.upper() == \"QUIT\":\n",
    "        print(\"감정 분석을 종료합니다.\")\n",
    "        break\n",
    "    predict2(sentence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, './senti7model2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabc08c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
